# Техническое видение проекта

## Технологии

- **Python 3.11+** — язык разработки
- **uv** — менеджер зависимостей и виртуальных окружений
- **ollama** — клиент для работы с LLM локально через Ollama
- **aiogram** (версия 3.x) — фреймворк для Telegram-бота с использованием polling
- **make** — для команд сборки и запуска проекта
- **python-dotenv** — для загрузки переменных окружения из `.env` файла
- **pydantic** — валидация данных и structured output для LLM

### Примечания

- Используется async/await (aiogram 3.x требует асинхронного кода)
- На старте проекта база данных не используется — диалоги хранятся только в памяти

## Принцип разработки

1. **KISS (Keep It Simple, Stupid)** — простота прежде всего
2. **YAGNI (You Aren't Gonna Need It)** — реализуем только необходимое, ничего "на будущее"
3. **Минимализм** — никакого оверинжиниринга, только базовый функционал
4. **Быстрая проверка идеи** — сначала рабочий прототип, потом улучшения (если потребуется)
5. **Код должен быть понятным** — без лишних абстракций и усложнений
6. **Следование PEP 8** — стандартный стиль кода Python

### Что не делаем на старте

- Тесты — только рабочий прототип
- Линтеры и форматтеры — можно добавить позже, если потребуется
- Сложная архитектура — никаких лишних слоев абстракции
- Функции "на будущее" — только то, что нужно прямо сейчас

## Структура проекта

```
03-aidd/
├── docs/
│   ├── idea.md
│   └── vision.md
├── src/
│   ├── main.py              # Точка входа, запуск бота
│   ├── bot.py               # Обработка сообщений Telegram
│   ├── llm.py               # Работа с LLM через Ollama
│   └── models.py            # Pydantic модели: структурированный вывод по калориям
├── .env.example             # Пример файла с переменными окружения
├── .env                     # Файл с реальными переменными (в .gitignore)
├── .gitignore               # Игнорируемые файлы
├── pyproject.toml           # Зависимости через uv
├── Makefile                 # Команды для сборки и запуска
└── README.md                # Базовая документация
```

### Описание модулей

- **main.py** — точка входа, инициализация и запуск бота
- **bot.py** — логика Telegram-бота, обработчики сообщений от пользователей
- **llm.py** — работа с LLM через Ollama
 - **models.py** — Pydantic модели для структурированного вывода по калориям; содержит `CalorieExtractionResult`, `CalorieResponse` и `CalorieType` (тип записи: потреблённые/потраченные), используется для валидации и сериализации данных между обработчиками и LLM

Пример ожидаемого JSON-ответа:

```json
{
  "calories": [
    {
      "date": "2024-10-30",
      "time": "12:00:00",
      "calorie_type": "BURN",
      "kkal": 560,
      "category": "велосипед"
    }
  ],
  "answer": "Записал вашу поездку на велосипеде на 560 ккал."
}
```

## Архитектура проекта

### Асинхронность

- **Все обработчики асинхронные** (async/await)
- **Параллельная обработка запросов:** один пользователь не блокирует других
- Aiogram использует event loop для параллельной обработки сообщений от разных пользователей
- Запросы к LLM выполняются асинхронно, не блокируя обработку других сообщений

### Поток работы

1. Пользователь отправляет сообщение в Telegram
2. `bot.py` получает сообщение через aiogram (асинхронно)
3. Для команды `/start`:
   - Выводится приветственное сообщение (async)
   - Очищается история диалога пользователя
4. Для команды `/balance`:
   - Бот формирует и отправляет отчёт по потреблённым и/или потраченным калориям (по умолчанию за текущий день или за указанный период)
   - Отчёт содержит суммарные значения и при необходимости список учтённых записей
5. Для обычного сообщения:
   1. В зависимости от типа сообщения (текст, изображение, голос) `bot.py` вызывает соответствующую LLM, которая отправляет запрос к Ollama с системным промптом (в зависимости от типа сообщения) и историей диалога (максимум 20 сообщений)
   2. LLM возвращает ответ боту
   3. Бот анализирует ответы и суммирует калории по типам
   4. Бот отправляет итоговый ответ пользователю (async) и обновляет историю диалога

### Хранение данных

- История диалога хранится в памяти в простом словаре: `{user_id: list[сообщений]}`
- Учтенные калории хранятся в простом словаре: `{user_id: list[CalorieExtractionResult]}`
- Формат сообщений: `{"role": "user"/"assistant", "content": "..."}`
- Ограничение: хранится максимум 20 последних сообщений для каждого пользователя
- При команде `/start` история пользователя очищается
- Доступ к словарю должен быть потокобезопасным (обычный dict в Python достаточно безопасен для чтения/записи в async контексте при использовании aiogram)

### Модули и их взаимодействие

- **main.py** — инициализирует бота, запускает async polling
- **bot.py** — асинхронная обработка команд и сообщений Telegram, управление историей диалога
- **llm.py** — асинхронная отправка запросов к Ollama, форматирование сообщений для LLM

## Модель данных

### История диалога

Структура данных:
```python
# Тип: dict[int, list[dict]]
# Ключ: user_id (int) - ID пользователя Telegram
# Значение: список сообщений

messages = [
    {"role": "system", "content": "системный промпт"},
    {"role": "user", "content": "текст сообщения"},
    {"role": "assistant", "content": "текст ответа"},
    ...
]
```

### Ограничения и правила

- Максимум 20 сообщений в истории для каждого пользователя
- При добавлении нового сообщения, если история превышает 20, удаляются самые старые сообщения
- Формат соответствует стандартному формату OpenAI API (role + content)
- Роли: `"user"` (сообщение пользователя) и `"assistant"` (ответ бота)
- Дополнительных данных о пользователях не хранится — только история диалога

## Работа с LLM

### Провайдер и модель

- **Провайдер:** Ollama (локальный)
- **Модель:** настраиваемая модель, установленная в Ollama (указывается в конфигурации)
- **Endpoint:** локальный демон Ollama OLLAMA_URL (по умолчанию `http://localhost:9999`)
- **Клиент:** `ollama` (локальный клиент/CLI или Python-обёртка для взаимодействия с Ollama)

### Системный промпт

- Роль: **Персональный тренер по фитнесу**
- Системный промпт задается один раз при инициализации
- Добавляется в каждый запрос к LLM как системное сообщение

### Формат запроса

Каждый запрос к LLM включает:
1. Системное сообщение с промптом роли
2. История диалога (до 20 последних сообщений)
3. Новое сообщение пользователя

### Параметры запроса

- **temperature:** 0.7 (баланс между креативностью и предсказуемостью)
- **max_tokens:** не ограничивается (или стандартное значение API)
- Другие параметры — значения по умолчанию

## Сценарии работы

### Сценарий 1: Первый запуск (команда /start)

1. Пользователь отправляет `/start` в Telegram
2. Бот выводит приветственное сообщение: "Привет! Я твой персональный тренер по фитнесу. Чем могу помочь?"
3. История диалога для пользователя очищается (или создается пустая запись)

### Сценарий 2: Обычный диалог

1. Пользователь отправляет текстовое сообщение
2. Бот добавляет сообщение пользователя в историю диалога
3. Бот вызывает LLM с:
   - Системным промптом (роль тренера)
   - Историей диалога (до 20 сообщений)
   - Новым сообщением пользователя
4. Получает ответ от LLM
5. Отправляет ответ пользователю в Telegram
6. Добавляет ответ ассистента в историю диалога
7. Если история превышает 20 сообщений, удаляются самые старые сообщения

### Сценарий 3: Нетекстовое сообщение

1. Пользователь отправляет фото, голосовое сообщение или другой тип контента
2. Для изображений: `bot.py` вызывает функцию обработки изображений, которая формирует и отправляет запрос к LLM с моделью, предназначенной для работы с изображениями, и системным промптом для извлечения информации о калориях; LLM возвращает структурированный результат (например, `CalorieExtractionResult` или `CalorieResponse`)
3. Для голосовых сообщений: бот при необходимости транскрибирует аудио, затем вызывает LLM с соответствующим промптом для голосовых сообщений и получает структурированный ответ
4. Для других типов контента: бот либо обрабатывает по аналогии, либо запрашивает у пользователя уточнение

### Поддерживаемые команды

- `/start` — приветствие и очистка истории диалога
- `/balance` — отправка пользователю отчёта по всем учтённым калориям
- Другие команды не обрабатываются (только текстовые сообщения)

## Подход к конфигурированию

### Переменные окружения (.env файл)

**Обязательные:**
- `TELEGRAM_BOT_TOKEN` — токен Telegram бота

**Опциональные (с значениями по умолчанию):**
- `OLLAMA_MODEL` — модель LLM (по умолчанию: `qwen3:8b`)
- `SYSTEM_PROMPT` — системный промпт для роли персонального тренера по фитнесу
- `TEMPERATURE` — параметр температуры для LLM (по умолчанию: `0.7`)
- `MAX_HISTORY_MESSAGES` — максимальное количество сообщений в истории диалога (по умолчанию: `20`)
- `IMAGE_MODEL` — модель/идентификатор для обработки изображений (по умолчанию: `OLLAMA_MODEL`)
- `AUDIO_MODEL` — модель/идентификатор для обработки аудио/транскрипции (по умолчанию: `OLLAMA_MODEL`)
- `IMAGE_PROMPT` — системный промпт для извлечения калорий из изображений (может переопределять `SYSTEM_PROMPT` для изображений)
- `AUDIO_PROMPT` — системный промпт для обработки транскрибированных голосовых сообщений (может переопределять `SYSTEM_PROMPT` для аудио)

### Файлы конфигурации

- `.env` — реальные переменные окружения (в .gitignore)
- `.env.example` — пример файла с описанием всех переменных и их назначением

### Загрузка конфигурации

- Использовать `python-dotenv` для загрузки переменных из `.env` файла
- Загрузка происходит при старте приложения в `main.py`
- Значения по умолчанию используются, если переменная не задана в `.env`

## Подход к логгированию

### Использование стандартного модуля

- **Модуль:** стандартный `logging` из Python (без дополнительных зависимостей)
- **Вывод:** консоль (stdout)
- **Формат:** простой текстовый формат с timestamp

### Уровни логирования

- **INFO** — основная информация (запуск/остановка бота, получение сообщений)
- **ERROR** — ошибки (проблемы с API, исключения)

### Что логировать

- Запуск и остановка бота
- Получение сообщений от пользователей (user_id, тип сообщения)
- Вызовы LLM (только факт вызова, без запросов и ответов)
- Ошибки при работе с Telegram API или OpenRouter API

### Что не логировать

- Содержимое сообщений пользователей
- Запросы к LLM (промпты, история диалога)
- Ответы от LLM
- Метрики и аналитику

### Принцип

Минимальное логирование только для понимания состояния бота и отслеживания ошибок. Без деталей содержимого сообщений и ответов LLM.

